# -*- coding: utf-8 -*-
"""
RAGç³»ç»Ÿæ ¸å¿ƒæ¨¡å—
æ•´åˆAgentã€å‘é‡å­˜å‚¨ã€æ•°æ®å¤„ç†ï¼Œæä¾›å®Œæ•´çš„æ£€ç´¢å¢å¼ºç”ŸæˆåŠŸèƒ½
"""

import requests
from typing import List, Dict, Any
from src.data.processor import DataProcessor
from src.core.vector_store import VectorStore
from src.agent.psychology_agent import PsychologyAgent
from src.config import *

class RAGSystem:
    def __init__(self):
        # åˆå§‹åŒ–ç»„ä»¶
        self.data_processor = DataProcessor()
        self.vector_store = VectorStore()
        self.psychology_agent = PsychologyAgent()
        
        # åˆå§‹åŒ–DeepSeek LLMé…ç½®
        self.deepseek_api_key = DEEPSEEK_API_KEY
        self.llm_url = f"{DEEPSEEK_BASE_URL}/chat/completions"
        
        # å¯¹è¯å†å²
        self.conversation_history = []
        
        # å¯¹è¯æŒç»­ç›‘æ§Agentï¼šè¿½è¸ªè¿ç»­æ— RAGçš„è½®æ•°
        self.no_rag_counter = 0  # è¿ç»­æ— RAGçš„è½®æ•°è®¡æ•°å™¨
        self.last_retrieval_docs = []  # ä¸Šæ¬¡æ£€ç´¢çš„æ–‡æ¡£ç¼“å­˜
        
        print("å¿ƒç†å’¨è¯¢RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š å¯¹è¯æŒç»­ç›‘æ§Agentå·²å¯ç”¨ï¼šè¿ç»­{MAX_NO_RAG_ROUNDS}è½®æ— RAGå°†å¼ºåˆ¶æ£€ç´¢")
    
    def build_knowledge_base(self, use_psychology_qa: bool = True, use_header_splitting: bool = True, clear_existing: bool = False) -> bool:
        """æ„å»ºå¿ƒç†å’¨è¯¢çŸ¥è¯†åº“"""
        try:
            print("å¼€å§‹æ„å»ºå¿ƒç†å’¨è¯¢çŸ¥è¯†åº“...")
            
            # æ¸…ç©ºç°æœ‰æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if clear_existing:
                self.vector_store.clear_collection()
            
            # å¤„ç†æ–‡æ¡£
            documents = self.data_processor.process_documents(use_psychology_qa, use_header_splitting)
            
            if not documents:
                print("æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡æ¡£")
                return False
            
            # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
            success = self.vector_store.add_documents(documents)
            
            if success:
                # æ˜¾ç¤ºçŸ¥è¯†åº“ä¿¡æ¯
                info = self.vector_store.get_collection_info()
                print(f"å¿ƒç†å’¨è¯¢çŸ¥è¯†åº“æ„å»ºå®Œæˆ: {info}")
                return True
            else:
                print("çŸ¥è¯†åº“æ„å»ºå¤±è´¥")
                return False
                
        except Exception as e:
            print(f"æ„å»ºçŸ¥è¯†åº“æ—¶å‡ºé”™: {e}")
            return False
    
    def generate_response(self, query: str, max_tokens: int = 1000) -> Dict[str, Any]:
        """æ™ºèƒ½ç”Ÿæˆå›ç­”ï¼ˆå«å¯¹è¯æŒç»­ç›‘æ§Agentï¼‰"""
        try:
            print(f"å¤„ç†æŸ¥è¯¢: {query}")
            
            # 1. ä½¿ç”¨AGENTåˆ†æç”¨æˆ·è¾“å…¥ï¼ˆä¼ é€’vector_storeä»¥æ”¯æŒå…ˆæ£€ç´¢å†å¼•å¯¼ï¼‰
            analysis = self.psychology_agent.analyze_user_input(query, self.conversation_history, self.vector_store)
            print(f"AGENTåˆ†æç»“æœ: {analysis}")
            
            # 2. å¯¹è¯æŒç»­ç›‘æ§Agentï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶è§¦å‘RAG
            force_rag = False
            if not analysis['need_rag']:
                self.no_rag_counter += 1
                print(f"ğŸ“Š æ— RAGè®¡æ•°å™¨: {self.no_rag_counter}/{MAX_NO_RAG_ROUNDS}")
                
                if self.no_rag_counter >= MAX_NO_RAG_ROUNDS:
                    force_rag = True
                    print(f"ğŸš¨ è§¦å‘å¼ºåˆ¶RAGæ£€ç´¢ï¼å·²è¿ç»­{self.no_rag_counter}è½®æœªä½¿ç”¨æ£€ç´¢")
                    # é‡æ–°è¿›è¡Œå®Œæ•´çš„åˆ†æï¼ˆåŒ…æ‹¬ä¸»é¢˜åˆ†ç±»å’Œqueryæ”¹å†™ï¼‰
                    analysis = self.psychology_agent.analyze_user_input(
                        query, self.conversation_history, self.vector_store, force_retrieval=True
                    )
                    print(f"ğŸ”„ å¼ºåˆ¶æ£€ç´¢åˆ†æç»“æœ: {analysis}")
            
            # 3. æ ¹æ®åˆ†æç»“æœå†³å®šæ˜¯å¦ä½¿ç”¨RAG
            if analysis['need_rag'] or force_rag:
                # ä½¿ç”¨RAGæ£€ç´¢
                search_queries = analysis['search_queries']  # è·å–æ‰€æœ‰ç”Ÿæˆçš„æŸ¥è¯¢è¯
                search_query = analysis['search_query']  # ä¿æŒå‘åå…¼å®¹
                topics = analysis.get('topics', [])
                topic = analysis['topic']  # ä¿æŒå‘åå…¼å®¹
                
                # å¯¹æ‰€æœ‰ç”Ÿæˆçš„æŸ¥è¯¢è¯éƒ½è¿›è¡Œæ£€ç´¢ï¼Œç„¶ååˆå¹¶ç»“æœ
                print(f"ğŸ” å¯¹ {len(search_queries)} ä¸ªæŸ¥è¯¢è¯è¿›è¡Œæ£€ç´¢: {search_queries}")
                relevant_docs = self._search_with_multiple_queries(search_queries, topics)
                
                # å¦‚æœæ˜¯å¼ºåˆ¶æ£€ç´¢ï¼Œåˆå¹¶ä¸Šæ¬¡ç¼“å­˜çš„æ£€ç´¢ç»“æœ
                if force_rag and self.last_retrieval_docs:
                    print(f"ğŸ”— åˆå¹¶ä¸Šæ¬¡æ£€ç´¢ç»“æœï¼ˆ{len(self.last_retrieval_docs)}ä¸ªæ–‡æ¡£ï¼‰ä¸æ–°æ£€ç´¢ç»“æœï¼ˆ{len(relevant_docs)}ä¸ªæ–‡æ¡£ï¼‰")
                    merged_docs = self._merge_and_sort_docs(self.last_retrieval_docs, relevant_docs)
                    relevant_docs = merged_docs
                    print(f"âœ… åˆå¹¶åå…± {len(relevant_docs)} ä¸ªæ–‡æ¡£")
                
                # é‡ç½®è®¡æ•°å™¨å¹¶ç¼“å­˜æœ¬æ¬¡æ£€ç´¢ç»“æœ
                self.no_rag_counter = 0
                self.last_retrieval_docs = relevant_docs.copy() if relevant_docs else []
                
                if relevant_docs:
                    # æ„å»ºä¸Šä¸‹æ–‡
                    context = self._build_context(relevant_docs)
                    
                    # æ„å»ºæç¤ºè¯
                    prompt = self._build_psychology_prompt(query, context, topic, analysis)
                    
                    # ç”Ÿæˆå›ç­”
                    answer = self._generate_response(prompt, query)
                    
                    # å‡†å¤‡æºæ–‡æ¡£ä¿¡æ¯
                    sources = []
                    for doc in relevant_docs:
                        sources.append({
                            'source': doc['metadata']['source'],
                            'topic': doc['metadata'].get('topic', ''),
                            'qa_id': doc['metadata'].get('qa_id', ''),
                            'similarity': doc['similarity']
                        })
                    
                    result = {
                        'success': True,
                        'response': answer,
                        'sources': sources,
                        'context_length': len(context),
                        'used_rag': True,
                        'topic': topic,
                        'search_query': search_query
                    }
                else:
                    # æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œç›´æ¥å›ç­”
                    prompt = self._build_direct_psychology_prompt(query, topic)
                    answer = self._generate_response(prompt, query)
                    
                    result = {
                        'success': True,
                        'response': answer,
                        'sources': [],
                        'used_rag': False,
                        'topic': topic,
                        'reason': "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„å¿ƒç†å’¨è¯¢æ¡ˆä¾‹"
                    }
            else:
                # ç›´æ¥å›ç­”ï¼Œä¸ä½¿ç”¨RAG
                prompt = self._build_direct_psychology_prompt(query)
                answer = self._generate_response(prompt, query)
                
                result = {
                    'success': True,
                    'response': answer,
                    'sources': [],
                    'used_rag': False,
                    'reason': "ç®€å•å¯¹è¯ï¼Œæ— éœ€æ£€ç´¢çŸ¥è¯†åº“"
                }
            
            # 3. æ›´æ–°å¯¹è¯å†å²
            self._update_conversation_history(query, result['response'])
            
            return result
            
        except Exception as e:
            print(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}")
            return {
                'success': False,
                'response': f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}",
                'sources': [],
                'reason': str(e)
            }
    
    def _search_with_multiple_queries(self, queries: List[str], topics: List[str] = None) -> List[Dict[str, Any]]:
        """å¯¹å¤šä¸ªæŸ¥è¯¢è¯è¿›è¡Œæ£€ç´¢ï¼Œåˆå¹¶æ‰€æœ‰ç»“æœå¹¶è¿”å›TOP K
        
        Args:
            queries: æŸ¥è¯¢è¯åˆ—è¡¨ï¼ˆåŒ…æ‹¬åŸå§‹é—®é¢˜å’Œæ”¹å†™çš„æŸ¥è¯¢è¯ï¼‰
            topics: ä¸»é¢˜åˆ—è¡¨
        
        Returns:
            åˆå¹¶å»é‡å¹¶æŒ‰ç›¸ä¼¼åº¦æ’åºçš„TOP Kæ–‡æ¡£åˆ—è¡¨
        """
        if not queries:
            return []
        
        all_docs = []
        doc_dict = {}  # ç”¨äºå»é‡ï¼Œkeyä¸ºæ–‡æ¡£å†…å®¹çš„å‰100ä¸ªå­—ç¬¦
        
        # å¯¹æ¯ä¸ªæŸ¥è¯¢è¯è¿›è¡Œæ£€ç´¢ï¼ˆæ˜¾å¼ä½¿ç”¨configä¸­çš„é˜ˆå€¼ï¼‰
        for i, query in enumerate(queries, 1):
            print(f"  æŸ¥è¯¢è¯ {i}/{len(queries)}: {query}")
            docs = self.vector_store.search(query, topics=topics, threshold=SIMILARITY_THRESHOLD)
            print(f"    æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£ï¼ˆé˜ˆå€¼={SIMILARITY_THRESHOLD}ï¼‰")
            
            # å»é‡å¹¶æ”¶é›†æ–‡æ¡£
            for doc in docs:
                doc_key = doc['content'][:100]  # ä½¿ç”¨å‰100ä¸ªå­—ç¬¦ä½œä¸ºå”¯ä¸€æ ‡è¯†
                if doc_key not in doc_dict:
                    doc_dict[doc_key] = doc
                    all_docs.append(doc)
                else:
                    # å¦‚æœæ–‡æ¡£å·²å­˜åœ¨ï¼Œä¿ç•™ç›¸ä¼¼åº¦æ›´é«˜çš„é‚£ä¸ª
                    if doc['similarity'] > doc_dict[doc_key]['similarity']:
                        # æ›´æ–°æ–‡æ¡£
                        doc_dict[doc_key] = doc
                        # åœ¨åˆ—è¡¨ä¸­æ‰¾åˆ°å¹¶æ›¿æ¢
                        for idx, existing_doc in enumerate(all_docs):
                            if existing_doc['content'][:100] == doc_key:
                                all_docs[idx] = doc
                                break
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        all_docs.sort(key=lambda x: x.get('similarity', 0), reverse=True)
        
        # è¿”å›TOP Kç»“æœ
        top_k_docs = all_docs[:TOP_K_RESULTS]
        print(f"âœ… å¤šæŸ¥è¯¢è¯æ£€ç´¢åˆå¹¶åå…± {len(all_docs)} ä¸ªæ–‡æ¡£ï¼ˆå»é‡åï¼‰ï¼Œè¿”å›TOP {len(top_k_docs)} ä¸ª")
        
        return top_k_docs
    
    def _merge_and_sort_docs(self, cached_docs: List[Dict[str, Any]], new_docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åˆå¹¶å¹¶æ’åºä¸Šæ¬¡ç¼“å­˜çš„æ–‡æ¡£å’Œæ–°æ£€ç´¢çš„æ–‡æ¡£
        
        Args:
            cached_docs: ä¸Šæ¬¡æ£€ç´¢ç¼“å­˜çš„æ–‡æ¡£
            new_docs: æœ¬æ¬¡æ–°æ£€ç´¢çš„æ–‡æ¡£
        
        Returns:
            åˆå¹¶å»é‡å¹¶æŒ‰ç›¸ä¼¼åº¦æ’åºçš„æ–‡æ¡£åˆ—è¡¨
        """
        # ä½¿ç”¨å­—å…¸å»é‡ï¼ˆåŸºäºæ–‡æ¡£å†…å®¹çš„å“ˆå¸Œï¼‰
        doc_dict = {}
        
        # å…ˆæ·»åŠ æ–°æ–‡æ¡£ï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
        for doc in new_docs:
            doc_key = doc['content'][:100]  # ä½¿ç”¨å‰100ä¸ªå­—ç¬¦ä½œä¸ºå”¯ä¸€æ ‡è¯†
            if doc_key not in doc_dict:
                doc_dict[doc_key] = doc
        
        # å†æ·»åŠ ç¼“å­˜æ–‡æ¡£
        for doc in cached_docs:
            doc_key = doc['content'][:100]
            if doc_key not in doc_dict:
                doc_dict[doc_key] = doc
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰ç›¸ä¼¼åº¦æ’åº
        merged_docs = list(doc_dict.values())
        merged_docs.sort(key=lambda x: x.get('similarity', 0), reverse=True)
        
        # é™åˆ¶åœ¨TOP_KèŒƒå›´å†…
        return merged_docs[:TOP_K_RESULTS]
    
    def _build_context(self, relevant_docs: List[Dict[str, Any]]) -> str:
        """æ„å»ºå¿ƒç†å’¨è¯¢ä¸Šä¸‹æ–‡"""
        context_parts = []
        
        for i, doc in enumerate(relevant_docs, 1):
            metadata = doc['metadata']
            topic = metadata.get('topic', 'æœªçŸ¥')
            qa_id = metadata.get('qa_id', 'æœªçŸ¥')
            
            context_parts.append(f"å‚è€ƒæ¡ˆä¾‹ {i} (ä¸»é¢˜: {topic}, ID: {qa_id}, ç›¸ä¼¼åº¦: {doc['similarity']:.3f}):")
            context_parts.append(doc['content'])
            context_parts.append("---")
        
        return "\n".join(context_parts)
    
    def _generate_response(self, system_prompt: str, user_query: str, include_history: bool = True) -> str:
        """ä½¿ç”¨DeepSeek LLMç”Ÿæˆå›ç­”"""
        try:
            headers = {
                "Authorization": f"Bearer {self.deepseek_api_key}",
                "Content-Type": "application/json"
            }
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«å¯¹è¯å†å²
            messages = []
            
            # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
            messages.append({
                "role": "system",
                "content": system_prompt
            })
            
            # å¦‚æœéœ€è¦åŒ…å«å†å²è®°å½•ä¸”å†å²è®°å½•å­˜åœ¨
            if include_history and self.conversation_history:
                # åªåŒ…å«æœ€è¿‘çš„6è½®å¯¹è¯ï¼ˆ12æ¡æ¶ˆæ¯ï¼‰ï¼Œé¿å…tokenè¿‡å¤š
                recent_history = self.conversation_history[-12:]
                messages.extend(recent_history)
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æŸ¥è¯¢
            messages.append({
                "role": "user", 
                "content": user_query
            })
            
            data = {
                "model": DEEPSEEK_MODEL,
                "messages": messages,
                "max_tokens": 1000,  # å‡å°‘tokenæ•°ï¼Œé¼“åŠ±ç®€çŸ­å›ç­”
                "temperature": 0.6,   # é™ä½éšæœºæ€§ï¼Œæ›´ç¨³å®š
                "top_p": 0.9
            }
            
            response = requests.post(self.llm_url, headers=headers, json=data)
            response.raise_for_status()
            
            result = response.json()
            if 'choices' in result and len(result['choices']) > 0:
                return result['choices'][0]['message']['content']
            else:
                print(f"LLM APIå“åº”æ ¼å¼é”™è¯¯: {result}")
                return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„å›ç­”ã€‚"
                
        except Exception as e:
            print(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}")
            return f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}"
    
    
    def _build_psychology_prompt(self, query: str, context: str, topic: str = None, analysis: Dict = None) -> str:
        """æ„å»ºå¿ƒç†å’¨è¯¢æç¤ºè¯"""
        prompt = f"""ä½ æ˜¯ä¸€ä½ç²¾é€šç†æƒ…è¡Œä¸ºç–—æ³•ï¼ˆRational Emotive Behavior Therapy,REBTï¼‰çš„å¿ƒç†å’¨è¯¢å¸ˆï¼Œèƒ½å¤Ÿåˆç†åœ°é‡‡ç”¨ç†æƒ…è¡Œä¸ºç–—æ³•ç»™æ¥è®¿è€…æä¾›ä¸“ä¸šåœ°æŒ‡å¯¼å’Œæ”¯æŒï¼Œç¼“è§£æ¥è®¿è€…çš„è´Ÿé¢æƒ…ç»ªå’Œè¡Œä¸ºååº”ï¼Œå¸®åŠ©ä»–ä»¬å®ç°ä¸ªäººæˆé•¿å’Œå¿ƒç†å¥åº·ã€‚
        
è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹å‚è€ƒçš„å¿ƒç†å’¨è¯¢å¯¹è¯æ¡ˆä¾‹ï¼Œå­¦ä¹ å…¶ä¸­åŠ©æ‰‹çš„å›åº”é£æ ¼å’ŒæŠ€å·§ï¼š
{context}

ç°åœ¨ï¼Œè¯·è¿ç”¨REBTç†è®ºå’Œå‚è€ƒæ¡ˆä¾‹ä¸­çš„é£æ ¼ï¼Œé’ˆå¯¹ç”¨æˆ·çš„å…·ä½“é—®é¢˜è¿›è¡Œå›åº”ã€‚

å›ç­”è¦æ±‚:
- ä¿æŒç®€çŸ­ï¼ˆ1-5å¥è¯ï¼‰
- ä»¥é¼“åŠ±å’Œå¼•å¯¼ä¸ºä¸»ï¼Œä½“ç°å…±æƒ…å’Œç†è§£
- è¯·æ³¨æ„å‚è€ƒæ¡ˆä¾‹ä¸­åŠ©æ‰‹çš„ç‰¹ç‚¹ï¼Œä½†è¦é’ˆå¯¹ç”¨æˆ·çš„å…·ä½“æƒ…å†µï¼Œä¸è¦æ··æ·†æ¡ˆä¾‹ä¸­çš„æƒ…å†µ
- å¯ä»¥ç”¨é—®é¢˜å¼•å¯¼ç”¨æˆ·è¿›ä¸€æ­¥æ€è€ƒå’Œè‡ªæˆ‘åçœ
- è¯­æ°”æ¸©æš–è‡ªç„¶ï¼Œå…·æœ‰ä¸“ä¸šæ€§

è¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸è¦é‡å¤æˆ–å¼•ç”¨æ¡ˆä¾‹å†…å®¹ã€‚
"""
        return prompt
    
    def _build_direct_psychology_prompt(self, query: str, topic: str = None) -> str:
        """æ„å»ºç›´æ¥å¿ƒç†å’¨è¯¢æç¤ºè¯ï¼ˆä¸ä½¿ç”¨RAGï¼‰"""
        prompt = f"""ä½ æ˜¯ä¸€ä½ç²¾é€šç†æƒ…è¡Œä¸ºç–—æ³•ï¼ˆRational Emotive Behavior Therapyï¼Œç®€ç§°REBTï¼‰çš„å¿ƒç†å’¨è¯¢å¸ˆï¼Œèƒ½å¤Ÿåˆç†åœ°é‡‡ç”¨ç†æƒ…è¡Œä¸ºç–—æ³•ç»™æ¥è®¿è€…æä¾›ä¸“ä¸šåœ°æŒ‡å¯¼å’Œæ”¯æŒï¼Œç¼“è§£æ¥è®¿è€…çš„è´Ÿé¢æƒ…ç»ªå’Œè¡Œä¸ºååº”ï¼Œå¸®åŠ©ä»–ä»¬å®ç°ä¸ªäººæˆé•¿å’Œå¿ƒç†å¥åº·ã€‚

ç†æƒ…è¡Œä¸ºæ²»ç–—ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š
ï¼ˆ1ï¼‰**æ£€æŸ¥éç†æ€§ä¿¡å¿µå’Œè‡ªæˆ‘æŒ«è´¥å¼æ€ç»´**ï¼šç†æƒ…è¡Œä¸ºç–—æ³•æŠŠè®¤çŸ¥å¹²é¢„è§†ä¸ºæ²»ç–—çš„"ç”Ÿå‘½"ï¼Œå› æ­¤ï¼Œå‡ ä¹ä»æ²»ç–—ä¸€å¼€å§‹ï¼Œåœ¨é—®é¢˜æ¢ç´¢é˜¶æ®µï¼Œå’¨è¯¢å¸ˆå°±ä»¥ç§¯æçš„ã€è¯´æœæ•™å¯¼å¼çš„æ€åº¦å¸®åŠ©æ¥è®¿è€…æ¢æŸ¥éšè—åœ¨æƒ…ç»ªå›°æ‰°åé¢çš„åŸå› ï¼ŒåŒ…æ‹¬æ¥è®¿è€…ç†è§£äº‹ä»¶çš„æ€ç»´é€»è¾‘ï¼Œäº§ç”Ÿæƒ…ç»ªçš„å‰å› åæœï¼Œå€Ÿæ­¤æ¥æ˜ç¡®é—®é¢˜çš„æ‰€åœ¨ã€‚å’¨è¯¢å¸ˆåšå®šåœ°æ¿€åŠ±æ¥è®¿è€…å»åçœè‡ªå·±åœ¨é­é‡åˆºæ¿€äº‹ä»¶åï¼Œåœ¨æ„Ÿåˆ°ç„¦è™‘ã€æŠ‘éƒæˆ–æ„¤æ€’å‰å¯¹è‡ªå·±"è¯´"äº†äº›ä»€ä¹ˆã€‚
ï¼ˆ2ï¼‰**ä¸éç†æ€§ä¿¡å¿µè¾©è®º**ï¼šå’¨è¯¢å¸ˆè¿ç”¨å¤šç§æŠ€æœ¯ï¼ˆä¸»è¦æ˜¯è®¤çŸ¥æŠ€æœ¯ï¼‰å¸®åŠ©æ¥è®¿è€…å‘éç†æ€§ä¿¡å¿µå’Œæ€ç»´è´¨ç–‘å‘éš¾ï¼Œè¯æ˜å®ƒä»¬çš„ä¸ç°å®ã€ä¸åˆç†ä¹‹å¤„ï¼Œè®¤è¯†å®ƒä»¬çš„å±å®³è¿›è€Œäº§ç”Ÿæ”¾å¼ƒè¿™äº›ä¸åˆç†ä¿¡å¿µçš„æ„¿æœ›å’Œè¡Œä¸ºã€‚
ï¼ˆ3ï¼‰**å¾—å‡ºåˆç†ä¿¡å¿µï¼Œå­¦ä¼šç†æ€§æ€ç»´**ï¼šåœ¨è¯†åˆ«å¹¶é©³å€’éç†æ€§ä¿¡å¿µçš„åŸºç¡€ä¸Šï¼Œå’¨è¯¢å¸ˆè¿›ä¸€æ­¥è¯±å¯¼ã€å¸®åŠ©æ¥è®¿è€…æ‰¾å‡ºå¯¹äºåˆºæ¿€æƒ…å¢ƒå’Œäº‹ä»¶çš„é€‚å®œçš„ã€ç†æ€§çš„ååº”ï¼Œæ‰¾å‡ºç†æ€§çš„ä¿¡å¿µå’Œå®äº‹æ±‚æ˜¯çš„ã€æŒ‡å‘é—®é¢˜è§£å†³çš„æ€ç»´é™ˆè¿°ï¼Œä»¥æ­¤æ¥æ›¿ä»£éç†æ€§ä¿¡å¿µå’Œè‡ªæˆ‘æŒ«è´¥å¼æ€ç»´ã€‚ä¸ºäº†å·©å›ºç†æ€§ä¿¡å¿µï¼Œå’¨è¯¢å¸ˆè¦å‘æ¥è®¿è€…åå¤æ•™å¯¼ï¼Œè¯æ˜ä¸ºä»€ä¹ˆç†æ€§ä¿¡å¿µæ˜¯åˆæƒ…åˆç†çš„ï¼Œå®ƒä¸éç†æ€§ä¿¡å¿µæœ‰ä»€ä¹ˆä¸åŒï¼Œä¸ºä»€ä¹ˆéç†æ€§ä¿¡å¿µå¯¼è‡´æƒ…ç»ªå¤±è°ƒï¼Œè€Œç†æ€§ä¿¡å¿µå¯¼è‡´è¾ƒç§¯æã€å¥åº·çš„ç»“æœã€‚
ï¼ˆ4ï¼‰**è¿ç§»åº”ç”¨æ²»ç–—æ”¶è·**ï¼šç§¯æé¼“åŠ±æ¥è®¿è€…æŠŠåœ¨æ²»ç–—ä¸­æ‰€å­¦åˆ°çš„å®¢è§‚ç°å®çš„æ€åº¦ï¼Œç§‘å­¦åˆç†çš„æ€ç»´æ–¹å¼å†…åŒ–æˆä¸ªäººçš„ç”Ÿæ´»æ€åº¦ï¼Œå¹¶åœ¨ä»¥åçš„ç”Ÿæ´»ä¸­åšæŒä¸æ‡ˆåœ°æŒ‰ç†æƒ…è¡Œä¸ºç–—æ³•çš„æ•™å¯¼æ¥è§£å†³æ–°çš„é—®é¢˜ã€‚

å›ç­”è¦æ±‚:
- ä¿æŒç®€çŸ­ï¼ˆ1-5å¥è¯ï¼‰
- è¿ç”¨REBTç†è®ºï¼Œå¸®åŠ©æ¥è®¿è€…è¯†åˆ«å’Œè´¨ç–‘éç†æ€§ä¿¡å¿µï¼Œä½†ä¸è¦æ˜ç¡®æåˆ°REBT
- ä»¥æ¸©æš–ã€ç†è§£ã€éè¯„åˆ¤çš„è¯­æ°”å›åº”
- ä»¥é¼“åŠ±å’Œå¼•å¯¼ä¸ºä¸»ï¼Œè€Œéç›´æ¥ç»™å»ºè®®
- ç”¨é—®é¢˜å¼•å¯¼ç”¨æˆ·æ€è€ƒå’Œè‡ªæˆ‘åçœ
- å¦‚æœæ˜¯ç®€å•é—®å€™ï¼Œä»¥å‹å–„è‡ªç„¶çš„æ–¹å¼å›åº”
- ä½“ç°å…±æƒ…å’Œç†è§£ï¼Œå…·æœ‰ä¸“ä¸šæ€§

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­æ°”è¦äº²åˆ‡è‡ªç„¶ã€‚
"""
        return prompt
    
    def _update_conversation_history(self, user_message: str, assistant_response: str):
        """æ›´æ–°å¯¹è¯å†å²"""
        self.conversation_history.append({
            'role': 'user',
            'content': user_message
        })
        self.conversation_history.append({
            'role': 'assistant', 
            'content': assistant_response
        })
        
        # ä¿æŒå¯¹è¯å†å²åœ¨åˆç†é•¿åº¦å†…ï¼ˆæœ€è¿‘10è½®å¯¹è¯ï¼‰
        if len(self.conversation_history) > 20:
            self.conversation_history = self.conversation_history[-20:]
    
    def clear_conversation_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_history = []
    
    def get_knowledge_base_info(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“ä¿¡æ¯"""
        return self.vector_store.get_collection_info()
    
    def test_query(self, query: str) -> None:
        """æµ‹è¯•æŸ¥è¯¢åŠŸèƒ½"""
        print(f"\n{'='*50}")
        print(f"æµ‹è¯•æŸ¥è¯¢: {query}")
        print(f"{'='*50}")
        
        result = self.generate_response(query)
        
        if result['success']:
            print(f"\nå›ç­”:")
            print(result['response'])
            
            print(f"\nå‚è€ƒæ¥æº:")
            for source in result['sources']:
                print(f"- {source['source']} (ç›¸ä¼¼åº¦: {source['similarity']:.3f})")
                if source.get('header'):
                    print(f"  æ ‡é¢˜: {source['header']}")
        else:
            print(f"æŸ¥è¯¢å¤±è´¥: {result.get('reason', 'æœªçŸ¥é”™è¯¯')}")


def main():
    """ä¸»å‡½æ•°"""
    rag = RAGSystem()
    
    # æ£€æŸ¥çŸ¥è¯†åº“çŠ¶æ€
    info = rag.get_knowledge_base_info()
    print(f"å½“å‰çŸ¥è¯†åº“çŠ¶æ€: {info}")
    
    # å¦‚æœçŸ¥è¯†åº“ä¸ºç©ºï¼Œæ„å»ºçŸ¥è¯†åº“
    if info.get('document_count', 0) == 0:
        print("çŸ¥è¯†åº“ä¸ºç©ºï¼Œå¼€å§‹æ„å»º...")
        success = rag.build_knowledge_base()
        if not success:
            print("çŸ¥è¯†åº“æ„å»ºå¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
            return
    
    # äº¤äº’å¼æŸ¥è¯¢
    print("\næ¬¢è¿ä½¿ç”¨MCPçŸ¥è¯†åº“RAGç³»ç»Ÿï¼")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºç¨‹åº")
    print("è¾“å…¥ 'info' æŸ¥çœ‹çŸ¥è¯†åº“ä¿¡æ¯")
    print("è¾“å…¥ 'rebuild' é‡æ–°æ„å»ºçŸ¥è¯†åº“")
    
    while True:
        try:
            query = input("\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
            
            if not query:
                continue
                
            if query.lower() in ['quit', 'exit', 'q']:
                print("å†è§ï¼")
                break
                
            if query.lower() == 'info':
                info = rag.get_knowledge_base_info()
                print(f"çŸ¥è¯†åº“ä¿¡æ¯: {info}")
                continue
                
            if query.lower() == 'rebuild':
                print("é‡æ–°æ„å»ºçŸ¥è¯†åº“...")
                success = rag.build_knowledge_base(clear_existing=True)
                if success:
                    print("çŸ¥è¯†åº“é‡å»ºå®Œæˆ")
                else:
                    print("çŸ¥è¯†åº“é‡å»ºå¤±è´¥")
                continue
            
            # å¤„ç†æŸ¥è¯¢
            rag.test_query(query)
            
        except KeyboardInterrupt:
            print("\n\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
            break
        except Exception as e:
            print(f"å¤„ç†è¾“å…¥æ—¶å‡ºé”™: {e}")


if __name__ == "__main__":
    main()

